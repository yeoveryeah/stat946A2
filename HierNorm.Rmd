---
title: "Hamiltonian Monte Carlo for Hierarchical Models"
author: "Ye Meng"
date: "`r format(Sys.Date(), format = '%B %d, %Y')`"
fontsize: 16pt
geometry: margin = 1in
linestretch: 1.5
urlcolor: blue
link-citations: yes
citecolor: blue
linkcolor: blue
output: bookdown::html_document2
---
\newcommand{\var}{\mathrm{var}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\N}{{\mathcal N}}
\newcommand{\iid}{\overset {\text{iid}} {\sim}}
\newcommand{\IG}{\text{Inv-Gamma}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, message = FALSE, warning = FALSE)
```


Three probabilistic programming packages offering Hamiltonian Monte Carlo(HMC) are Stan, PyMC3 and NumPyro. They will be implemented on a hierarchical model, a type of multi-level models with hierarchical structure to the parameters, and systematically compared for efficiency and accuracy. 

# Hierachical Model 

The [random effects model](https://en.wikipedia.org/wiki/Random_effects_model) is given by 

$$
\begin{equation}
y_i = \mu_i + \varepsilon_i 
(\#eq:rnd)
\end{equation}
$$

where the random effects $\mu_i \sim \N(\lambda, \tau^2)$ specifies the cluster means, and the random error term $\varepsilon_i \sim \N(0,1)$ is the deviation of observation from the cluster mean.

From sampling perspective, the population of interest is divided into non-overlapping clusters, where $y_i$'s are observed value in cluster $i$ and $\mu_i$ are underlying cluster means. The parameter of interest of this model is the underlying population mean ($\lambda$), the population variance ($\tau^2$) and the residual variance ($\sigma^2$).

To compensate the lack of independence of errors for observations within clusters, the equivalent model, which clarifies the hierarchical nature of the model, is given by:
$$
y_i \mid \mu_i \iid \N (\mu_i, \sigma^2),\quad i=1,\dots,N\\
\mu_i \mid \lambda,\tau \iid \N(\lambda, \tau^2),\quad  \sigma^2\sim \IG(1,1)\\
\lambda \sim \N(0, 100),\quad \tau^2 \sim \IG(1,1)\\
$$

The rationale behind the choice of their distributions are discussed:

* It's further assumed that the sizes of the observed objects in each cluster are relatively large such that it's safe to have observed cluster means $y_i$ being normally distributed.

* The weakly weakly informative prior prior for $\lambda$ is adpoted due to the lack of information of the population mean.

* The population variance ( $\tau^2$) and residual variance ( $\sigma^2$) are assumed to follow the Inverse Gamma distribution,which is a conjugate prior of a normal distribution with known mean.

In computing, the non-centered parameterization will be embraced to shift the correlation with the parameters to the hyperparameters and to improve the efficiency.

$$
\lambda = 10\cdot\tilde{\lambda}, \ \tilde{\lambda} \sim \N (0,1)
$$

On the other hand, the random effects model \@ref(eq:rnd) can be parameterized as follows
$$
y_i = \lambda + \tilde{\mu_i} + \varepsilon_i
$$
where $\tilde{\mu_i}$ measures the deviation of cluster mean from the population mean and $\tilde{\mu_i} \sim \N(0,\tau^2)$. Assuming $\tilde{\mu_i} \indep \varepsilon_i$ will yield $y_i \sim \N(\lambda, \tau^2+\sigma^2)$ such that the $\mu_i$ can be managed by specifying $\lambda$ and $\tau^2$ in programming.

The motivations and examples of reparameterization are discussed in the [Section 22.7](https://mc-stan.org/docs/2_23/stan-users-guide/reparameterization-section.html) of the Stan User's guide, and it also applies to PyMC3 and NumPy cases.

The generated data are prepared as follows for the simulation study.

```{r Sim, eval=TRUE}
set.seed(42) # for reproducibility 
N <- 20 # number of clusters
true_lam <- 3 # true population mean
true_tau2 <- 1.5 # true cross -cluster variance
true_sig2 <- 1.2 # true inter-cluster variance 
true_mu <-true_lam + sqrt(true_tau2)*rnorm(N) # true cluster means
y <- true_mu + sqrt(true_sig2)*rnorm(N) # observed data input for models
```


# Stan

This section will offer a step-by-step guide for estimating the parameters from the simulated data in Stan. Installation guidelines and useful online resources are listed in Section \@ref(sec:resstan)

## Stan Modelling language

The hierarchical model can be translated into Stan modeling language. Note that the hyper-prior for $\lambda$ is not specified in the `model` block, Stan will automatically nominate the uniform prior on its range, i.e. the real numbers, to it.

```{r Stanfile, eval=FALSE}
// HierNorm.stan
data {
  int<lower=1> N; // number of clusters in the population
  real y[N]; // observed cluster means
}
parameters {
  real lambda_tilda;
  // variance are restricted to be positve:
  real<lower=0.00001> tau_sq; // cross-cluster variance
  real<lower=0.00001> sigma_sq; // inter-cluster varaince
}
transformed parameters {
  real<lower=0.00001> y_sd = sqrt(tau_sq + sigma_sq);
  real<lower=0.00001> tau = sqrt(tau_sq);
  // implies lambda ~ normal(0, sd = 10)
  real lambda = 10*lambda_tilda;
}
model {
  // hyper-priors:
  tau_sq ~ inv_gamma(1, 1);
  lambda_tilda ~ std_normal();
  // priors:
  sigma_sq ~ inv_gamma(1, 1);
  // likelihood:
  y ~ normal(lambda, y_sd);
}
```


## Stan Model Fitting 

The stanmodel object from the model specified in the Stan modeling language can be complied with the `stan_model()` function:

```{r StanSetup, eval=TRUE}
require(rstan)
rstan_options(auto_write = TRUE)
# Compile the model
hnm_mod <- stan_model(file = "HierNorm.stan") 
```

### Correctness Check of Stan Model

To ensure integrity, the Stan code shall be tested in the first place. For generic MCMC, checking that log-posterior is correct will suffice, which can be realized with `log_prob` and `expose_stan_functions` functions in rstan package.

The stan model can be associated with the observations for log-posterior check by sampling for one iteration. For this purpose, the warnings produced by the `hnm_fit_init` can be ignored for now. 

```{r StanInit, eval=TRUE}
#  1. format data: 
#   list with elements named exactly as "data" block in Stan
hnm_data <- list(N = N, y = y)
#  2. associate data with model:
#   to do this, MCMC sample for one iteration.
hnm_fit_init <- sampling(hnm_mod, data = hnm_data, iter = 1, 
                         verbose = TRUE, chains = 1)
```

In R, the log-posterior are calculated with the `logpost` function defined below.

```{r lpR, eval=TRUE}
#' Log-Posterior Function
#'
#' @description  `logpost` returns the log-posterior of the hierarchical model given the parameter values and observations
#'  
#' @param lambda_tilda A real number, the hyperparameter
#' @param tau_sq A positive number, the hyperparameter
#' @param sigma_sq A positive number
#' @param y A vector of reals 
#' @return The log-posterior values
#' @details The hyper-prior are given by tau_sq ~ InverseGamma(1,1) and lambda_tilda ~ N(0,1) such that lambda ~ N(0,100).
#' The prior are given by sigma_sq InverseGamma(1,1);
#' The likelihood are given by the Normal(lambda, sqrt(sigma_sq + tau_sq))
#' @import invgamma 
#' @export
logpost <- function(lambda_tilda, tau_sq, sigma_sq, y) {
  require(invgamma)
  # Hyper-prior:
  lp <- dnorm(lambda_tilda, log = TRUE)
  lp <- lp + dinvgamma(tau_sq, shape = 1, scale = 1, log = TRUE)
  # Prior:
  lp <- lp + dinvgamma(sigma_sq, shape = 1, scale = 1, log = TRUE)
  # likelihood:
  lambda <- 10*lambda_tilda
  y_sd <- sqrt(sigma_sq+tau_sq)
  lp <- lp + sum(dnorm(y, mean = lambda, sd = y_sd, log = TRUE)) 
  # output: 
  lp
}
```


With arbitrarily generated model parameters, it's expected that the R log-posterior and Stan log-posterior at these values differ by a constant. This constant is the summation of the constant additive term of the log-posterior, which are dropped by default in Stan.

$$
\log\left(
\underbrace{(\frac{1}{\sqrt{2\pi}})^N}_{y_i \sim \N(\mu_i,\sigma^2)}
\cdot \underbrace{(\frac{1^1}{\Gamma(1)})^2}_{\sigma^2, \tau^2 \sim \IG(1,1)}
\cdot \underbrace{\frac{1}{\sqrt{2\pi}}}_{\lambda \sim \N(0,1)}
\right)
 = -(N+1)\log(\sqrt{2\pi})
$$

```{r Stanlp, eval=TRUE}
# Simulate parameter values:
nsim <- 5 # number of simulations to conduct
# randomly generated parameter values to check the correctness
Pars <- replicate(n = nsim, expr = {
    list(lambda_tilda = rexp(1), 
         tau_sq = rexp(1)*5, sigma_sq = rexp(1)*2)
}, simplify = FALSE)

# log-posterior calculation in R:
lpR <- sapply(1:nsim, function(ii) {
  lambda_tilda <- Pars[[ii]]$lambda_tilda
  tau_sq <- Pars[[ii]]$tau_sq
  sigma_sq <- Pars[[ii]]$sigma_sq
  logpost(lambda_tilda = lambda_tilda,
          tau_sq = tau_sq, sigma_sq = sigma_sq, y = hnm_data$y)
})

# log-posterior calculation in Stan:
lpStan <- sapply(1:nsim, function(ii) {
  upars <- unconstrain_pars(object = hnm_fit_init, Pars[[ii]])
  log_prob(object = hnm_fit_init, upars = upars, adjust_transform = FALSE)
})

lpR-lpStan # return exactly same constant

-(N +1 )*log(sqrt(2*pi)) # Theoretical difference
```

### Stan Model Fitting

After being checked, the HMC sampling are implemented to make parameter inference.
```{r StanRun}
# sampling
nsamples <- 1e4
stan_time <- system.time(
  hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
                      control = list(adapt_delta = 0.99)))
# extract MCMC samples
hnm_post <- extract(hnm_fit)
```

In this step, the `hnm_fit` would give some of preruntime warnings and runtime warnings that can be managed by adjusting control factor. This [guide](https://mc-stan.org/misc/warnings.html) provides instructions to handle with these warnings. 

```{r, echo=FALSE, eval=TRUE}
#saveRDS(list(hnm_fit= hnm_fit, stan_time = stan_time), file = "hnm_post.rds")
require(rstan)
hnm_mcmc <- readRDS('hnm_post.rds')
hnm_fit <- hnm_mcmc$hnm_fit # MCMC samples
stan_time <- hnm_mcmc$stan_time
hnm_post <- extract(hnm_fit)
rm(hnm_mcmc)
```

### Convergence Diagnostic

Before making parameter inference, the posterior predictive checks are employed to evaluate the fit of the model. 

```{r StanTrace, eval=TRUE, fig.width=10, fig.height=4,fig.cap='Trace plot of parameters from MCMC samples'}
theta <- c("lambda","tau_sq","sigma_sq")
stan_trace(hnm_fit, pars = theta) # trace plot 
```

The chains completely overlap with each other, which is evidence that the MCMC has converged. 

```{r StanDens, eval=TRUE, fig.width=10, fig.height=4,fig.cap='Density estimates'}
stan_dens(hnm_fit, pars = theta, separate_chains = TRUE) #kernel density estimates
```

### Parameter Inference

```{r lambda, eval=TRUE, fig.width=10, fig.height=4, fig.cap="Posterior of parameter of interest with true parameter value"}
# plot the histogram of the MCMC samples of population mean
lambda_post = hnm_post$lambda
sig2_post = hnm_post$sigma_sq
tau2_post = hnm_post$tau_sq
par(mfrow =c(1,3))
# population mean:
hist(lambda_post, breaks = 80, probability = TRUE, col = 'white',
     main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_lam, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(lambda_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
# Population variance
hist(tau2_post, breaks = 80, probability = TRUE, col = 'white',
     main = 'Posterior of Population Variance', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_tau2, lty = 1, lwd = 2, col = 'red') # True population variance
abline(v = mean(tau2_post), lty = 2, lwd = 2, col = 'orange') # Posterior variance
# Residual Variance
hist(sig2_post, breaks = 80, probability = TRUE, col = 'white',
     main = 'Posterior of Residual Variance', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_sig2, lty = 1, lwd = 2, col = 'red') # True residual variance
abline(v = mean(sig2_post), lty = 2, lwd = 2, col = 'orange') # Posterior residual vairance
legend("right", col = c("red", "orange"), c("True Parameter","Mean of Posterior"), 
       lty = c(1,2), cex = 1, bty='n')# add legends
```



## Resources for Stan {#sec:resstan}

* [Stan Installation Instructions](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started) provides a quick tutorial for downloading, installing, and getting started with RStan on all platforms.

* [Stan User's Guide](https://mc-stan.org/docs/2_22/stan-users-guide/index.html) is the official userâ€™s guide for Stan and provides examples for coding statistical models in Stan. 

* [Stan Reference Manual](https://mc-stan.org/docs/2_23/reference-manual/index.html) specifies the programming language, inference algorithms and posterior analysis tools.

* [Stan Functions Reference](https://mc-stan.org/docs/2_23/functions-reference/index.html) provides a list of defined functions and distributions in the Stan math library and available in the Stan programming language.

# PyMC3




# NumPyro