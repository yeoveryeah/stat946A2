})
# log-posterior calculation in Stan:
lpStan <- sapply(1:nsim, function(ii) {
upars <- unconstrain_pars(object = hnm_fit_init, Pars[[ii]])
log_prob(object = hnm_fit_init, upars = upars, adjust_transform = FALSE)
})
lpR-lpStan # return exactly same constant
-(2*N +1 )*log(sqrt(2*pi)) # Theoretical difference
# sampling
nsamples <- 1e4
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
# extract MCMC samples
hnm_post <- extract(hnm_fit)
# extract MCMC samples
hnm_post <- extract(hnm_fit)
theta <- c("lambda","tau_sq","sigma_sq")
stan_trace(hnm_fit, pars = theta) # trace plot
stan_dens(hnm_fit, pars = theta, separate_chains = TRUE) #kernel density estimates
# plot the histogram of the MCMC samples of population mean
lambda_post = hnm_post$lambda
hist(lambda_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_lam, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(hnm_post$lambda), lty = 2, lwd = 2, col = 'orange') # Posterior mean
legend("right", col = c("red", "orange"), c("True Mean","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
saveRDS(list(hnm_fit= hnm_fit, stan_time = stan_time), file = "hnm_post.rds")
# Compile the model
hnm_mod <- stan_model(file = "HierNorm.stan")
#' Log-Posterior Function
#'
#' @description  `logpost` returns the log-posterior of the hierarchical model given the parameter values and observations
#'
#' @param lambda A real number, the hyperparameter
#' @param mu_raw A vector of reals at the same length of y
#' @param tau_sq A positive number, the hyperparameter
#' @param sigma_sq A positive number
#' @param y A vector of reals
#' @return The log-posterior values
#' @details The hyper-prior are given by tau_sq ~ InverseGamma(1,1) and lambda ~ N(0,1).
#' The prior are given by mu_raw ~ N(0,1 ) and sigma_sq InverseGamma(1,1), while the non-centered parameterization are implemented here.
#' The likelihood are given by the Normal(mu, sigma_sq)
#' @import invgamma
#' @export
logpost <- function(lambda_tilda, mu_tilda, tau_sq, sigma_sq, y) {
require(invgamma)
# Hyper-prior:
lp <- dnorm(lambda_tilda, log = TRUE)
lp <- lp + dinvgamma(tau_sq, shape = 1, scale = 1, log = TRUE)
# Prior:
lp <- lp + dinvgamma(sigma_sq, shape = 1, scale = 1, log = TRUE)
lp <- lp + sum(dnorm(mu_tilda, log = TRUE))
# likelihood:
lambda <- 10*lambda_tilda
mu <- lambda + sqrt(tau_sq) * mu_tilda
lp <- lp + sum(dnorm(y, mean = mu, sd = sqrt(sigma_sq), log = TRUE))
# output:
lp
}
# Simulate parameter values:
nsim <- 5 # number of simulations to conduct
# randomly generated parameter values to check the correctness
Pars <- replicate(n = nsim, expr = {
list(lambda_tilda = rexp(1), mu_tilda = rnorm(N),
tau_sq = rexp(1)*5, sigma_sq = rexp(1)*2)
}, simplify = FALSE)
# log-posterior calculation in R:
lpR <- sapply(1:nsim, function(ii) {
lambda_tilda <- Pars[[ii]]$lambda_tilda
mu_tilda <- Pars[[ii]]$mu_tilda
tau_sq <- Pars[[ii]]$tau_sq
sigma_sq <- Pars[[ii]]$sigma_sq
logpost(lambda_tilda = lambda_tilda, mu_tilda = mu_tilda,
tau_sq = tau_sq, sigma_sq = sigma_sq, y = hnm_data$y)
})
# log-posterior calculation in Stan:
lpStan <- sapply(1:nsim, function(ii) {
upars <- unconstrain_pars(object = hnm_fit_init, Pars[[ii]])
log_prob(object = hnm_fit_init, upars = upars, adjust_transform = FALSE)
})
lpR-lpStan # return exactly same constant
-(2*N +1 )*log(sqrt(2*pi)) # Theoretical difference
# sampling
nsamples <- 1e4
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
# extract MCMC samples
hnm_post <- extract(hnm_fit)
# extract MCMC samples
hnm_post <- extract(hnm_fit)
theta <- c("lambda","tau_sq","sigma_sq")
stan_trace(hnm_fit, pars = theta) # trace plot
stan_dens(hnm_fit, pars = theta, separate_chains = TRUE) #kernel density estimates
# plot the histogram of the MCMC samples of population mean
lambda_post = hnm_post$lambda
hist(lambda_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_lam, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(hnm_post$lambda), lty = 2, lwd = 2, col = 'orange') # Posterior mean
legend("right", col = c("red", "orange"), c("True Mean","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
set.seed(42) # for reproducibility
N <- 20 # number of clusters
true_lam <- 3 # true population mean
true_tau2 <- 1.5 # true cross -cluster variance
true_sig2 <- 1.2 # true inter-cluster variance
true_mu <-true_lam + sqrt(true_tau2)*rnorm(N) # true cluster means
y <- true_mu + sqrt(true_sig2)*rnorm(N) # observed data input for models
require(rstan)
rstan_options(auto_write = TRUE)
# Compile the model
hnm_mod <- stan_model(file = "HierNorm.stan")
#  1. format data:
#   list with elements named exactly as "data" block in Stan
hnm_data <- list(N = N, y = y)
#  2. associate data with model:
#   to do this, MCMC sample for one iteration.
hnm_fit_init <- sampling(hnm_mod, data = hnm_data, iter = 1,
verbose = TRUE, chains = 1)
#' Log-Posterior Function
#'
#' @description  `logpost` returns the log-posterior of the hierarchical model given the parameter values and observations
#'
#' @param lambda A real number, the hyperparameter
#' @param mu_raw A vector of reals at the same length of y
#' @param tau_sq A positive number, the hyperparameter
#' @param sigma_sq A positive number
#' @param y A vector of reals
#' @return The log-posterior values
#' @details The hyper-prior are given by tau_sq ~ InverseGamma(1,1) and lambda ~ N(0,1).
#' The prior are given by mu_raw ~ N(0,1 ) and sigma_sq InverseGamma(1,1), while the non-centered parameterization are implemented here.
#' The likelihood are given by the Normal(mu, sigma_sq)
#' @import invgamma
#' @export
logpost <- function(lambda_tilda, mu_tilda, tau_sq, sigma_sq, y) {
require(invgamma)
# Hyper-prior:
lp <- dnorm(lambda_tilda, log = TRUE)
lp <- lp + dinvgamma(tau_sq, shape = 1, scale = 1, log = TRUE)
# Prior:
lp <- lp + dinvgamma(sigma_sq, shape = 1, scale = 1, log = TRUE)
lp <- lp + sum(dnorm(mu_tilda, log = TRUE))
# likelihood:
lambda <- 10*lambda_tilda
mu <- lambda + sqrt(tau_sq) * mu_tilda
lp <- lp + sum(dnorm(y, mean = mu, sd = sqrt(sigma_sq), log = TRUE))
# output:
lp
}
# Simulate parameter values:
nsim <- 5 # number of simulations to conduct
# randomly generated parameter values to check the correctness
Pars <- replicate(n = nsim, expr = {
list(lambda_tilda = rexp(1), mu_tilda = rnorm(N),
tau_sq = rexp(1)*5, sigma_sq = rexp(1)*2)
}, simplify = FALSE)
# log-posterior calculation in R:
lpR <- sapply(1:nsim, function(ii) {
lambda_tilda <- Pars[[ii]]$lambda_tilda
mu_tilda <- Pars[[ii]]$mu_tilda
tau_sq <- Pars[[ii]]$tau_sq
sigma_sq <- Pars[[ii]]$sigma_sq
logpost(lambda_tilda = lambda_tilda, mu_tilda = mu_tilda,
tau_sq = tau_sq, sigma_sq = sigma_sq, y = hnm_data$y)
})
# log-posterior calculation in Stan:
lpStan <- sapply(1:nsim, function(ii) {
upars <- unconstrain_pars(object = hnm_fit_init, Pars[[ii]])
log_prob(object = hnm_fit_init, upars = upars, adjust_transform = FALSE)
})
lpR-lpStan # return exactly same constant
-(2*N +1 )*log(sqrt(2*pi)) # Theoretical difference
#' Log-Posterior Function
#'
#' @description  `logpost` returns the log-posterior of the hierarchical model given the parameter values and observations
#'
#' @param lambda A real number, the hyperparameter
#' @param mu_raw A vector of reals at the same length of y
#' @param tau_sq A positive number, the hyperparameter
#' @param sigma_sq A positive number
#' @param y A vector of reals
#' @return The log-posterior values
#' @details The hyper-prior are given by tau_sq ~ InverseGamma(1,1) and lambda ~ N(0,1).
#' The prior are given by mu_raw ~ N(0,1 ) and sigma_sq InverseGamma(1,1), while the non-centered parameterization are implemented here.
#' The likelihood are given by the Normal(mu, sigma_sq)
#' @import invgamma
#' @export
logpost <- function(lambda_tilda, tau_sq, sigma_sq, y) {
require(invgamma)
# Hyper-prior:
lp <- dnorm(lambda_tilda, log = TRUE)
lp <- lp + dinvgamma(tau_sq, shape = 1, scale = 1, log = TRUE)
# Prior:
lp <- lp + dinvgamma(sigma_sq, shape = 1, scale = 1, log = TRUE)
# likelihood:
lambda <- 10*lambda_tilda
y_sd <- sqrt(sigma_sq+tau_sq)
lp <- lp + sum(dnorm(y, mean = lambda, sd = y_sd, log = TRUE))
# output:
lp
}
# Simulate parameter values:
nsim <- 5 # number of simulations to conduct
# randomly generated parameter values to check the correctness
Pars <- replicate(n = nsim, expr = {
list(lambda_tilda = rexp(1), mu_tilda = rnorm(N),
tau_sq = rexp(1)*5, sigma_sq = rexp(1)*2)
}, simplify = FALSE)
# log-posterior calculation in R:
lpR <- sapply(1:nsim, function(ii) {
lambda_tilda <- Pars[[ii]]$lambda_tilda
mu_tilda <- Pars[[ii]]$mu_tilda
tau_sq <- Pars[[ii]]$tau_sq
sigma_sq <- Pars[[ii]]$sigma_sq
logpost(lambda_tilda = lambda_tilda, mu_tilda = mu_tilda,
tau_sq = tau_sq, sigma_sq = sigma_sq, y = hnm_data$y)
})
# Simulate parameter values:
nsim <- 5 # number of simulations to conduct
# randomly generated parameter values to check the correctness
Pars <- replicate(n = nsim, expr = {
list(lambda_tilda = rexp(1),
tau_sq = rexp(1)*5, sigma_sq = rexp(1)*2)
}, simplify = FALSE)
# log-posterior calculation in R:
lpR <- sapply(1:nsim, function(ii) {
lambda_tilda <- Pars[[ii]]$lambda_tilda
tau_sq <- Pars[[ii]]$tau_sq
sigma_sq <- Pars[[ii]]$sigma_sq
logpost(lambda_tilda = lambda_tilda,
tau_sq = tau_sq, sigma_sq = sigma_sq, y = hnm_data$y)
})
# log-posterior calculation in Stan:
lpStan <- sapply(1:nsim, function(ii) {
upars <- unconstrain_pars(object = hnm_fit_init, Pars[[ii]])
log_prob(object = hnm_fit_init, upars = upars, adjust_transform = FALSE)
})
lpR-lpStan # return exactly same constant
-(2*N +1 )*log(sqrt(2*pi)) # Theoretical difference
# Simulate parameter values:
nsim <- 5 # number of simulations to conduct
# randomly generated parameter values to check the correctness
Pars <- replicate(n = nsim, expr = {
list(lambda_tilda = rexp(1),
tau_sq = rexp(1)*5, sigma_sq = rexp(1)*2)
}, simplify = FALSE)
# log-posterior calculation in R:
lpR <- sapply(1:nsim, function(ii) {
lambda_tilda <- Pars[[ii]]$lambda_tilda
tau_sq <- Pars[[ii]]$tau_sq
sigma_sq <- Pars[[ii]]$sigma_sq
logpost(lambda_tilda = lambda_tilda,
tau_sq = tau_sq, sigma_sq = sigma_sq, y = hnm_data$y)
})
# log-posterior calculation in Stan:
lpStan <- sapply(1:nsim, function(ii) {
upars <- unconstrain_pars(object = hnm_fit_init, Pars[[ii]])
log_prob(object = hnm_fit_init, upars = upars, adjust_transform = FALSE)
})
lpR-lpStan # return exactly same constant
-(N +1 )*log(sqrt(2*pi)) # Theoretical difference
# Simulate parameter values:
nsim <- 5 # number of simulations to conduct
# randomly generated parameter values to check the correctness
Pars <- replicate(n = nsim, expr = {
list(lambda_tilda = rexp(1),
tau_sq = rexp(1)*5, sigma_sq = rexp(1)*2)
}, simplify = FALSE)
# log-posterior calculation in R:
lpR <- sapply(1:nsim, function(ii) {
lambda_tilda <- Pars[[ii]]$lambda_tilda
tau_sq <- Pars[[ii]]$tau_sq
sigma_sq <- Pars[[ii]]$sigma_sq
logpost(lambda_tilda = lambda_tilda,
tau_sq = tau_sq, sigma_sq = sigma_sq, y = hnm_data$y)
})
# log-posterior calculation in Stan:
lpStan <- sapply(1:nsim, function(ii) {
upars <- unconstrain_pars(object = hnm_fit_init, Pars[[ii]])
log_prob(object = hnm_fit_init, upars = upars, adjust_transform = FALSE)
})
lpR-lpStan # return exactly same constant
-(N +1 )*log(sqrt(2*pi)) # Theoretical difference
# sampling
nsamples <- 1e4
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
# extract MCMC samples
hnm_post <- extract(hnm_fit)
theta <- c("lambda","tau_sq","sigma_sq")
stan_trace(hnm_fit, pars = theta) # trace plot
stan_dens(hnm_fit, pars = theta, separate_chains = TRUE) #kernel density estimates
# plot the histogram of the MCMC samples of population mean
lambda_post = hnm_post$lambda
hist(lambda_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_lam, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(hnm_post$lambda), lty = 2, lwd = 2, col = 'orange') # Posterior mean
legend("right", col = c("red", "orange"), c("True Mean","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
saveRDS(list(hnm_fit= hnm_fit, stan_time = stan_time), file = "hnm_post.rds")
stan_time
sig2_post = hnm_post$sigma_sq
hist(sig2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_sig2, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(sig2_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
legend("right", col = c("red", "orange"), c("True Mean","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
tau2_post = hnm_post$tau_sq
hist(tau2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_tau2, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(tau2_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
legend("right", col = c("red", "orange"), c("True Mean","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
# plot the histogram of the MCMC samples of population mean
lambda_post = hnm_post$lambda
sig2_post = hnm_post$sigma_sq
tau2_post = hnm_post$tau_sq
par(mfrow =c(1,3))
hist(lambda_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_lam, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(lambda_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
hist(sig2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_sig2, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(sig2_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
hist(tau2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_tau2, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(tau2_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
legend("right", col = c("red", "orange"), c("True Mean","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
# plot the histogram of the MCMC samples of population mean
lambda_post = hnm_post$lambda
sig2_post = hnm_post$sigma_sq
tau2_post = hnm_post$tau_sq
par(mfrow =c(1,3))
hist(lambda_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_lam, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(lambda_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
hist(sig2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_sig2, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(sig2_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
hist(tau2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_tau2, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(tau2_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
legend("right", col = c("red", "orange"), c("True Mean","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
# plot the histogram of the MCMC samples of population mean
lambda_post = hnm_post$lambda
sig2_post = hnm_post$sigma_sq
tau2_post = hnm_post$tau_sq
par(mfrow =c(1,3))
hist(lambda_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_lam, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(lambda_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
hist(sig2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_sig2, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(sig2_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
hist(tau2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_tau2, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(tau2_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
legend("right", col = c("red", "orange"), c("True Mean","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
# plot the histogram of the MCMC samples of population mean
lambda_post = hnm_post$lambda
sig2_post = hnm_post$sigma_sq
tau2_post = hnm_post$tau_sq
par(mfrow =c(1,3))
# population mean:
hist(lambda_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Mean', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_lam, lty = 1, lwd = 2, col = 'red') # True population mean
abline(v = mean(lambda_post), lty = 2, lwd = 2, col = 'orange') # Posterior mean
# Population variance
hist(tau2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Population Variance', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_tau2, lty = 1, lwd = 2, col = 'red') # True population variance
abline(v = mean(tau2_post), lty = 2, lwd = 2, col = 'orange') # Posterior variance
# Residual Variance
hist(sig2_post, breaks = 80, probability = TRUE, col = 'white',
main = 'Posterior of Residual Variance', xlab = expression(lambda), ylim=c(0,0.8))
abline(v = true_sig2, lty = 1, lwd = 2, col = 'red') # True residual variance
abline(v = mean(sig2_post), lty = 2, lwd = 2, col = 'orange') # Posterior residual vairance
legend("right", col = c("red", "orange"), c("True Parameter","Mean of Posterior"),
lty = c(1,2), cex = 1, bty='n')# add legends
require(reticulate)
require(reticulate)
use_python('/usr/local/bin/python3')
knitr::opts_chunk$set(eval = FALSE, message = FALSE, warning = FALSE)
#saveRDS(list(hnm_fit= hnm_fit, stan_time = stan_time), file = "hnm_post.rds")
require(rstan)
hnm_mcmc <- readRDS('hnm_post.rds')
hnm_fit <- hnm_mcmc$hnm_fit # MCMC samples
stan_time <- hnm_mcmc$stan_time
hnm_post <- extract(hnm_fit)
rm(hnm_mcmc)
stan_time
stan_time$elapsed
stan_time[3]
as.numeric(stan_time[3])
set.seed(42) # for reproducibility
N <- 20 # number of clusters
true_lam <- 3 # true population mean
true_tau2 <- 1.5 # true cross -cluster variance
true_sig2 <- 1.2 # true inter-cluster variance
true_mu <-true_lam + sqrt(true_tau2)*rnorm(N) # true cluster means
y <- true_mu + sqrt(true_sig2)*rnorm(N) # observed data input for models
y
stan_time
set.seed(42) # for reproducibility
N <- 20 # number of clusters
true_lam <- 3 # true population mean
true_tau2 <- 1.5 # true cross -cluster variance
true_sig2 <- 1.2 # true inter-cluster variance
true_mu <-true_lam + sqrt(true_tau2)*rnorm(N) # true cluster means
y <- true_mu + sqrt(true_sig2)*rnorm(N) # observed data input for models
require(rstan)
rstan_options(auto_write = TRUE)
# Compile the model
hnm_mod <- stan_model(file = "HierNorm.stan")
#  1. format data:
#   list with elements named exactly as "data" block in Stan
hnm_data <- list(N = N, y = y)
#  2. associate data with model:
#   to do this, MCMC sample for one iteration.
hnm_fit_init <- sampling(hnm_mod, data = hnm_data, iter = 1,
verbose = TRUE, chains = 1)
# sampling
nsamples <- 1e4
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
# extract MCMC samples
hnm_post <- extract(hnm_fit)
stan_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
stan_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
stan_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
stan_time <- end_time - start_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
stan_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
start_time <- Sys.time()
stan_time <- system.time(
hnm_fit <- sampling(hnm_mod, data = hnm_data, iter = nsamples,
control = list(adapt_delta = 0.99)))
end_time <- Sys.time()
end_time - start_time
